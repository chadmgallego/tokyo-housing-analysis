{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c231c16c-3eac-4af1-83cc-52f4ac38f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, sqlite3, sys, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a76aa-4798-4073-9d16-a8ad6e4ad30b",
   "metadata": {},
   "source": [
    "## Data Collection Class: `TokyoHousingScraper`\n",
    "> This class handles all **data ingestion** tasks, including:\n",
    "> - Initializing local SQLite database connection\n",
    "> - Scraping Tokyo listings HTML from *SUUMO.jp*\n",
    "> - Parsing station information and other housing metrics from raw HTML\n",
    "> - Building a robust housing dataset which includes features such as: `title`, `floor`, `area`, `rent`, `deposit`, etc.\n",
    "> - Storing the resulting dataset in local SQLite table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d697d0f5-b73a-47fb-b6d5-22c9be23033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TokyoHousingScraper to:\n",
    "# - Scrape Tokyo housing listings from SUUMO.jp\n",
    "# - Collect listings, parse property details\n",
    "# - Store the results in SQLite database\n",
    "\n",
    "class TokyoHousingScraper:\n",
    "    def __init__(self, db, base_url, url):\n",
    "        # Initialize DB connection\n",
    "        self.db = db\n",
    "        self.conn = sqlite3.connect(self.db)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        \n",
    "        # Base URL and starting page\n",
    "        self.base_url = base_url\n",
    "        self.url = url\n",
    "        \n",
    "    def scrape_listings(self):\n",
    "        # Define list for storing HTML\n",
    "        self.listings = list()\n",
    "        \n",
    "        # Iterate through all pages of listings\n",
    "        while True:\n",
    "            try:\n",
    "                response = requests.get(next_page) #this will only work after the first page\n",
    "            except:\n",
    "                response = requests.get(self.url) # starting url \n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "\t\t    \n",
    "            # Each listing = cassetteitem div\n",
    "            cassettes = soup.select('div.cassetteitem')\n",
    "            self.listings.extend(cassettes)\n",
    "            #print(len(self.listings))\n",
    "            \n",
    "            # Find next page link (pagination)\n",
    "            try:\n",
    "                current_page = soup.find('li', class_ = 'pagination-current')\n",
    "                next_page_path = current_page.find_next_siblings('li')[1]\n",
    "                next_page = self.base_url + next_page_path.select_one('a').get('href')\n",
    "            except: break #no more pages to comb through\n",
    "            \n",
    "        print(f'{len(self.listings)} properties were successfully gathered!')\n",
    "        \n",
    "    def parse_station_info(self, item):\n",
    "        \n",
    "        # Extract station information (names, distances, nearest, average).\n",
    "        # Returns tuple: \n",
    "            # (stations_str, nearest_station, distance_to_nearest_station, avg_distance).\n",
    "        \n",
    "        # Get raw station blocks\n",
    "        stations_list = item.select('li.cassetteitem_detail-col2 div.cassetteitem_detail-text')\n",
    "        \n",
    "        # If there is no station information, return None\n",
    "        if not stations_list:\n",
    "            return (None, None, None, None)\n",
    "        else: pass\n",
    "        \n",
    "        # Remove empty tags\n",
    "        stations_list = [s for s in stations_list if s != '']\n",
    "        \n",
    "        # All stations as a single string (for DB storage)\n",
    "        self.stations_str = \",\".join([station.get_text().strip() for station in stations_list])\n",
    "        \n",
    "        # Extract stations and distances with regex\n",
    "        stations_dict = {\n",
    "            # All listed stations\n",
    "            'stations': [\n",
    "                re.findall(r'/(?P<station>.*?)\\s*歩', station.get_text().strip())[0] \n",
    "                for station in stations_list\n",
    "                if re.findall(r'/(?P<station>.*?)\\s*歩', station.get_text().strip())\n",
    "            ],\n",
    "            \n",
    "            'distances': [\n",
    "                re.findall(r'\\d+', station.get_text().strip())[0]\n",
    "                for station in stations_list\n",
    "                if re.findall(r'\\d+', station.get_text().strip())\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Compute distance to nearest station\n",
    "        self.distance_to_nearest_station = min([int(dist) for dist in stations_dict['distances']])\n",
    "        nearest_idx = stations_dict['distances'].index(str(self.distance_to_nearest_station))\n",
    "        self.nearest_station = stations_dict['stations'][nearest_idx]\n",
    "        \n",
    "        # Compute average distance to surrounding stations\n",
    "        self.avg_distance = np.mean([float(dist) for dist in stations_dict['distances']])\n",
    "        \n",
    "        return self.stations_str, self.nearest_station, self.distance_to_nearest_station, self.avg_distance\n",
    "        \n",
    "    def parse_sublistings(self, sub):\n",
    "        # Parse a single sublisting HTML element and extract key property details.\n",
    "    \n",
    "        # Parameters \n",
    "            # sub : bs4.element.Tag\n",
    "            # A BeautifulSoup <tr> or sublisting element containing rent, fees, floor, \n",
    "            # floor plan, and area information.\n",
    "\n",
    "        # Returns\n",
    "            # A pandas Series with the following fields:\n",
    "            # - url: full URL for the sublisting\n",
    "            # - rent: rent amount (string)\n",
    "            # - management_fee: management fee (string)\n",
    "            # - deposit: deposit amount (string)\n",
    "            # - key_money: key money / deposit (string)\n",
    "            # - floor: floor number or description (string)\n",
    "            # - floor_plan: floor plan (string, e.g., 1R, 2LDK)\n",
    "            # - area: area in square meters (string)\n",
    "        \n",
    "        sublisting_tags = {\n",
    "            # Get URL tags for each sublisting\n",
    "            'url_tag': sub.select_one('td.ui-text--midium.ui-text--bold a'),\n",
    "\n",
    "            # Pricing fields\n",
    "            'rent_tag': sub.select_one('span.cassetteitem_price.cassetteitem_price--rent'),\n",
    "            'management_fee_tag': sub.select_one('span.cassetteitem_price.cassetteitem_price--administration'),\n",
    "            'deposit_tag': sub.select_one('span.cassetteitem_price.cassetteitem_price--deposit'),\n",
    "            'key_money_tag': sub.select_one('span.cassetteitem_price.cassetteitem_price--gratuity'),\n",
    "\n",
    "            # Property details\n",
    "            'floor_cells': sub.select('tr.js-cassette_link td'),\n",
    "            'floor_plan_tag': sub.select_one('span.cassetteitem_madori'),\n",
    "            'area_tag': sub.select_one('span.cassetteitem_menseki')\n",
    "        }\n",
    "\n",
    "        sublisting_data = {\n",
    "            # Combine base_url with endpoints for complete URLs\n",
    "            'url': self.base_url[:-1] + sublisting_tags['url_tag'].get('href') if sublisting_tags['url_tag'] else None,\n",
    "            \n",
    "            # Pricing fields\n",
    "            'rent': sublisting_tags['rent_tag'].get_text().strip() if sublisting_tags['rent_tag'] else None,\n",
    "            'management_fee': sublisting_tags['management_fee_tag'].get_text().strip() if sublisting_tags['management_fee_tag'] else None,\n",
    "            'deposit': sublisting_tags['deposit_tag'].get_text().strip() if sublisting_tags['deposit_tag'] else None,\n",
    "            'key_money': sublisting_tags['key_money_tag'].get_text().strip() if sublisting_tags['key_money_tag'] else None,\n",
    "                \n",
    "            # Floor information is in the third <td> cell of the sublisting row.\n",
    "            # If fewer than 3 cells exist, set to None\n",
    "            'floor': (\n",
    "                sublisting_tags['floor_cells'][2].get_text().strip() \n",
    "                if len(sublisting_tags['floor_cells']) >= 3 \n",
    "                else None\n",
    "            ),\n",
    "\n",
    "            'floor_plan': sublisting_tags['floor_plan_tag'].get_text().strip() if sublisting_tags['floor_plan_tag'] else None,\n",
    "            'area': sublisting_tags['area_tag'].get_text().strip() if sublisting_tags['area_tag'] else None\n",
    "        }\n",
    "\n",
    "        # Return pandas DataFrame for added columns\n",
    "        return pd.Series(sublisting_data)\n",
    "    \n",
    "    def build_housing_dataset(self):\n",
    "    \t\n",
    "        # Parse all previously scraped listings into a structured dataset\n",
    "        # and load the results to SQLite.\n",
    "\n",
    "        # First pass: collect all relevant BeautifulSoup tags for each listing.\n",
    "        # We separate \"tag extraction\" from \"text/value extraction\" so:\n",
    "        #   1) Each CSS selector runs only once per listing\n",
    "        #   2) Missing fields can be handled consistently later\n",
    "        # select_one() returns a single value\n",
    "        # select() returns a list (empty if no tags exist)\n",
    "        building_tags = [\n",
    "            {\n",
    "                # Core building info\n",
    "                'img_tag': item.select_one('div.cassetteitem_object img'),\n",
    "                'title_tag': item.select_one('div.cassetteitem_content-title'),\n",
    "                'address_tag': item.select_one('li.cassetteitem_detail-col1'),\n",
    "                'building_cells': item.select('li.cassetteitem_detail-col3 div'),\n",
    "                \n",
    "                # Pre-computed station information\n",
    "                # (returns tuple: stations_str, nearest_station, nearest_dist, avg_dist)              \n",
    "                'stations_info': self.parse_station_info(item),\n",
    "\n",
    "                # Building sublistings\n",
    "                'sublisting_tags': item.select('tr.js-cassette_link')\n",
    "            }\n",
    "            for item in self.listings\n",
    "        ]\n",
    "        \n",
    "        # Second pass: convert tags into clean, normalized Python values\n",
    "        # All conditional checks rely on truthiness:\n",
    "        #   - BeautifulSoup tag → truthy\n",
    "        #   - None or empty list → falsy\n",
    "        building_data = [\n",
    "            {\n",
    "                # Image URL (some buildings omit images)\n",
    "                'img': building['img_tag'].get('rel') if building['img_tag'] else None,\n",
    "                \n",
    "                # Text-based fields\n",
    "                'title': building['title_tag'].get_text().strip() if building['title_tag'] else None,\n",
    "                'address': building['address_tag'].get_text().strip() if building['address_tag'] else None,\n",
    "                \n",
    "                # Building metadata\n",
    "                'building_age': building['building_cells'][0].get_text().strip() if building['building_cells'] else None,\n",
    "                'building_size': (\n",
    "                    building['building_cells'][1].get_text().strip() \n",
    "                    if len(building['building_cells']) >= 2 \n",
    "                    else None\n",
    "                ),\n",
    "                \n",
    "                # Station-related features (already parsed and computed)\n",
    "                'stations': building['stations_info'][0],\n",
    "                'nearest_station': building['stations_info'][1],\n",
    "                'distance_to_nearest_station': building['stations_info'][2],\n",
    "                'avg_distance_to_stations': building['stations_info'][3],\n",
    "\n",
    "                # Building sublistings (which will be parsed with pandas)\n",
    "                'sublistings': building['sublisting_tags']\n",
    "            }\n",
    "            for building in building_tags\n",
    "        ]\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        housing_data_df = pd.DataFrame(building_data)\n",
    "\n",
    "        # Explode sublistings into unique rows\n",
    "        housing_data_df = housing_data_df.explode('sublistings')\n",
    "\n",
    "        # Parse sublistings and retrieve listing-level metrics\n",
    "        housing_data_df[['url', 'rent', 'management_fee', 'deposit', 'key_money', 'floor', 'floor_plan', 'area']] = (\n",
    "            housing_data_df['sublistings'].apply(self.parse_sublistings)\n",
    "        )\n",
    "\n",
    "        # Drop sublistings column after parsing \n",
    "        housing_data_df.drop(columns = ['sublistings'], inplace = True)\n",
    "\n",
    "        # Load raw, structured housing data to SQLite table\n",
    "        housing_data_df.to_sql(name = 'HOUSING_DATA_RAW', con = self.conn, if_exists = 'replace', index = False)\n",
    "        \n",
    "        # Close the DB connection\n",
    "        self.conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
