{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c231c16c-3eac-4af1-83cc-52f4ac38f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, sqlite3, sys, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a76aa-4798-4073-9d16-a8ad6e4ad30b",
   "metadata": {},
   "source": [
    "## Data Collection Class: `TokyoHousingScraper`\n",
    "> This class handles all **data ingestion** tasks, including:\n",
    "> - Initializing local SQLite database connection\n",
    "> - Scraping Tokyo listings HTML from *SUUMO.jp*\n",
    "> - Parsing station information and other housing metrics from raw HTML\n",
    "> - Building a robust housing dataset which includes features such as: `title`, `floor`, `area`, `rent`, `deposit`, etc.\n",
    "> - Storing the resulting dataset in local SQLite table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2183ea4b-827c-4569-aba4-dce2926f616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TokyoHousingScraper to:\n",
    "# - Scrape Tokyo housing listings from Suumo.jp\n",
    "# - Collect listings, parse property details\n",
    "# - Store the results in SQLite database\n",
    "\n",
    "class TokyoHousingScraper:\n",
    "    def __init__(self, db, base_url, url):\n",
    "        # Initialize DB connection\n",
    "        self.db = db\n",
    "        self.conn = sqlite3.connect(self.db)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        \n",
    "        # Base URL and starting page\n",
    "        self.base_url = base_url\n",
    "        self.url = url\n",
    "        \n",
    "    def scrape_listings(self):\n",
    "        # Define list for storing HTML\n",
    "        self.listings = list()\n",
    "        \n",
    "        # Iterate through all pages of listings\n",
    "        while True:\n",
    "            try:\n",
    "                response = requests.get(next_page) #this will only work after the first page\n",
    "            except:\n",
    "                response = requests.get(self.url) # starting url \n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "\t\t    \n",
    "            # Each listing = cassetteitem div\n",
    "            cassettes = soup.select('div.cassetteitem')\n",
    "            self.listings.extend(cassettes)\n",
    "            #print(len(self.listings))\n",
    "            \n",
    "            # Find next page link (pagination)\n",
    "            try:\n",
    "                current_page = soup.find('li', class_ = 'pagination-current')\n",
    "                next_page_path = current_page.find_next_siblings('li')[1]\n",
    "                next_page = self.base_url + next_page_path.select_one('a').get('href')\n",
    "            except: break #no more pages to comb through\n",
    "            \n",
    "        #print(f'{len(self.listings)} listings were successfully gathered!')\n",
    "        \n",
    "    def parse_station_info(self, item):\n",
    "        \n",
    "        # Extract station information (names, distances, nearest, average).\n",
    "        # Returns tuple: \n",
    "            # (stations_str, nearest_station, distance_to_nearest_station, avg_distance).\n",
    "        \n",
    "        # Get raw station blocks\n",
    "        stations_list = item.select('li.cassetteitem_detail-col2 div.cassetteitem_detail-text')\n",
    "        \n",
    "        # If there is no station information, return None\n",
    "        if not stations_list:\n",
    "            return (None, None, None, None)\n",
    "        else: pass\n",
    "        \n",
    "        # Remove empty tags\n",
    "        stations_list = [s for s in stations_list if s != '']\n",
    "        \n",
    "        # All stations as a single string (for DB storage)\n",
    "        self.stations_str = \",\".join([station.get_text().strip() for station in stations_list])\n",
    "        \n",
    "        # Extract stations and distances with regex\n",
    "        stations_dict = {\n",
    "            # All listed stations\n",
    "            'stations': [\n",
    "                re.findall(r'/(?P<station>.*?)\\s*歩', station.get_text().strip())[0] \n",
    "                for station in stations_list\n",
    "                if re.findall(r'/(?P<station>.*?)\\s*歩', station.get_text().strip())\n",
    "            ],\n",
    "            \n",
    "            'distances': [\n",
    "                re.findall(r'\\d+', station.get_text().strip())[0]\n",
    "                for station in stations_list\n",
    "                if re.findall(r'\\d+', station.get_text().strip())\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Compute distance to nearest station\n",
    "        self.distance_to_nearest_station = min([int(dist) for dist in stations_dict['distances']])\n",
    "        nearest_idx = stations_dict['distances'].index(str(self.distance_to_nearest_station))\n",
    "        self.nearest_station = stations_dict['stations'][nearest_idx]\n",
    "        \n",
    "        # Compute average distance to surrounding stations\n",
    "        self.avg_distance = np.mean([float(dist) for dist in stations_dict['distances']])\n",
    "        \n",
    "        return self.stations_str, self.nearest_station, self.distance_to_nearest_station, self.avg_distance\n",
    "        \n",
    "    def build_housing_dataset(self):\n",
    "    \t\n",
    "        # Parse all previously scraped listings into a structured dataset\n",
    "        # and load the results to SQLite.\n",
    "\n",
    "        # First pass: collect all relevant BeautifulSoup tags for each listing.\n",
    "        # We separate \"tag extraction\" from \"text/value extraction\" so:\n",
    "        #   1) Each CSS selector runs only once per listing\n",
    "        #   2) Missing fields can be handled consistently later\n",
    "        # select_one() returns a single value\n",
    "        # select() returns a list (empty if no tags exist)\n",
    "        listing_tags = [\n",
    "            {\n",
    "                # Core listing info\n",
    "                'img_tag': item.select_one('div.cassetteitem_object img'),\n",
    "                'title_tag': item.select_one('div.cassetteitem_content-title'),\n",
    "                'address_tag': item.select_one('li.cassetteitem_detail-col1'),\n",
    "\n",
    "                # Pricing information\n",
    "                'rent_tag': item.select_one('span.cassetteitem_price.cassetteitem_price--rent'),\n",
    "                'management_fee_tag': item.select_one('span.cassetteitem_price.cassetteitem_price--administration'),\n",
    "                'deposit_tag': item.select_one('span.cassetteitem_price.cassetteitem_price--deposit'),\n",
    "                'key_money_tag': item.select_one('span.cassetteitem_price.cassetteitem_price--gratuity'),\n",
    "\n",
    "                # Property details\n",
    "                'floor_cells': item.select('div.cassetteitem-item tr.js-cassette_link td'),\n",
    "                'floor_plan_tag': item.select_one('span.cassetteitem_madori'),\n",
    "                'area_tag': item.select_one('span.cassetteitem_menseki'),\n",
    "                'building_cells': item.select('li.cassetteitem_detail-col3 div'),\n",
    "                \n",
    "                # Pre-computed station information\n",
    "                # (returns tuple: stations_str, nearest_station, nearest_dist, avg_dist)              \n",
    "                'stations_info': self.parse_station_info(item)\n",
    "            }\n",
    "            for item in self.listings\n",
    "        ]\n",
    "\n",
    "        # Second pass: convert tags into clean, normalized Python values\n",
    "        # All conditional checks rely on truthiness:\n",
    "        #   - BeautifulSoup tag → truthy\n",
    "        #   - None or empty list → falsy\n",
    "        housing_data = [\n",
    "            {\n",
    "                # Image URL (some listings omit images)\n",
    "                'img': listing['img_tag'].get('rel') if listing['img_tag'] else None,\n",
    "                \n",
    "                # Text-based fields\n",
    "                'title': listing['title_tag'].get_text().strip() if listing['title_tag'] else None,\n",
    "                'address': listing['address_tag'].get_text().strip() if listing['address_tag'] else None,\n",
    "                \n",
    "                # Pricing fields\n",
    "                'rent': listing['rent_tag'].get_text().strip() if listing['rent_tag'] else None,\n",
    "                'management_fee': listing['management_fee_tag'].get_text().strip() if listing['management_fee_tag'] else None,\n",
    "                'deposit': listing['deposit_tag'].get_text().strip() if listing['deposit_tag'] else None,\n",
    "                'key_money': listing['key_money_tag'].get_text().strip() if listing['key_money_tag'] else None,\n",
    "                \n",
    "                # Floor information is stored in a fixed table layout:\n",
    "                # index 2 corresponds to the floor number when present\n",
    "                'floor': (\n",
    "                    listing['floor_cells'][2].get_text().strip() \n",
    "                    if len(listing['floor_cells']) >= 3 \n",
    "                    else None\n",
    "                ),\n",
    "\n",
    "                'floor_plan': listing['floor_plan_tag'].get_text().strip() if listing['floor_plan_tag'] else None,\n",
    "                'area': listing['area_tag'].get_text().strip() if listing['area_tag'] else None,\n",
    "                \n",
    "                # Building metadata\n",
    "                'building_age': listing['building_cells'][0].get_text().strip() if listing['building_cells'] else None,\n",
    "                'building_size': (\n",
    "                    listing['building_cells'][1].get_text().strip() \n",
    "                    if len(listing['building_cells']) >= 2 \n",
    "                    else None\n",
    "                ),\n",
    "                \n",
    "                # Station-related features (already parsed and computed)\n",
    "                'stations': listing['stations_info'][0],\n",
    "                'nearest_station': listing['stations_info'][1],\n",
    "                'distance_to_nearest_station': listing['stations_info'][2],\n",
    "                'avg_distance_to_stations': listing['stations_info'][3]\n",
    "            }\n",
    "            for listing in listing_tags\n",
    "        ]\n",
    "        \n",
    "        # Convert to DataFrame and load to SQLite\n",
    "        housing_data_df = pd.DataFrame(housing_data)\n",
    "        housing_data_df.to_sql(name = 'HOUSING_DATA', con = self.conn, if_exists = 'replace', index = False)\n",
    "        \n",
    "        # Close the DB connection\n",
    "        self.conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
